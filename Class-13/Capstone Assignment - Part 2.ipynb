{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question & Answering with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "Previously we saw that the model told us how to to change the tire, however we had to manually provide it with the relevant data and provide the contex ourselves. We explored the approach to leverage the model availabe under Bedrock and ask questions based on it's knowledge learned during training as well as providing manual context. While that approach works with short documents or single-ton applications, it fails to scale to enterprise level question answering where there could be large enterprise documents which cannot all be fit into the prompt sent to the model. \n",
    "\n",
    "### Pattern\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "### Challenges\n",
    "- How to manage large document(s) that exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "#### Prepare documents\n",
    "![Embeddings](./images/Embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "#### Ask question\n",
    "![Question](./images/Chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "#### Dataset\n",
    "To explain this architecture pattern we are using the documents from IRS. These documents explain topics such as:\n",
    "- Original Issue Discount (OID) Instruments\n",
    "- Reporting Cash Payments of Over $10,000 to IRS\n",
    "- Employer's Tax Guide\n",
    "\n",
    "#### Persona\n",
    "Let's assume a persona of a layman who doesn't have an understanding of how IRS works and if some actions have implications or not.\n",
    "\n",
    "The model will try to answer from the documents in easy language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In order to follow the RAG approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude V1 available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Document Loader**: PDF Loader available through LangChain\n",
    "\n",
    "  This is the loader that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: FAISS available through LangChain\n",
    "\n",
    "  In this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "- **Index**: VectorIndex\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pypdf==4.1.0\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from pypdf==4.1.0) (4.12.2)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
      "Collecting faiss-cpu==1.8.0\n",
      "  Downloading faiss_cpu-1.8.0-cp38-cp38-win_amd64.whl (14.5 MB)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting tiktoken==0.6.0\n",
      "  Downloading tiktoken-0.6.0-cp38-cp38-win_amd64.whl (798 kB)\n",
      "Collecting sqlalchemy==2.0.28\n",
      "  Downloading SQLAlchemy-2.0.28-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\agust\\anaconda3\\lib\\site-packages (from faiss-cpu==1.8.0) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from sqlalchemy==2.0.28) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from sqlalchemy==2.0.28) (1.0.0)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.9.11-cp38-cp38-win_amd64.whl (274 kB)\n",
      "Collecting requests>=2.26.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.10)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp38-cp38-win_amd64.whl (101 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.6.0) (1.26.4)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-community) (0.2.16)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-community) (0.1.136)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-community) (5.4.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.39 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-community) (0.2.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.15.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (20.3.0)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain-community) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.39->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.10)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\agust\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\agust\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (2.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\agust\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\agust\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Installing collected packages: charset-normalizer, requests, typing-inspect, sqlalchemy, marshmallow, regex, dataclasses-json, tiktoken, langchain-community, faiss-cpu\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.7\n",
      "    Uninstalling SQLAlchemy-1.4.7:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.7\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "Successfully installed charset-normalizer-3.4.0 dataclasses-json-0.6.7 faiss-cpu-1.8.0 langchain-community-0.2.17 marshmallow-3.22.0 regex-2024.9.11 requests-2.32.3 sqlalchemy-2.0.28 tiktoken-0.6.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain>=0.1.11\n",
    "%pip install pypdf==4.1.0\n",
    "%pip install langchain-community faiss-cpu==1.8.0 tiktoken==0.6.0 sqlalchemy==2.0.28\n",
    "%pip install boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime', region_name='eu-west-2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure langchain\n",
    "\n",
    "We begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude for text generation and Amazon Titan for text embedding.\n",
    "\n",
    "Note: It is possible to choose other models available with Bedrock. You can replace the `model_id` as follows to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"amazon.titan-text-express-v1\")`\n",
    "\n",
    "Check [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) for Available text generation and embedding models Ids under Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "#llm = Bedrock(model_id=\"amazon.titan-text-lite-v1\", client=boto3_bedrock, model_kwargs={})\n",
    "llm = Bedrock(model_id=\"amazon.titan-text-express-v1\", client=boto3_bedrock, model_kwargs={})\n",
    "#bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note: As an exercise. If you have time, update the cell above so that it uses the \"new/appropriate\" version/style of code, so that the deprication issue/warning is resolved.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's first download some of the files to build our document store. For this example we will be using public IRS documents from [here](https://www.irs.gov/publications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1544.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p15.pdf\",\n",
    "    \"https://www.irs.gov/pub/irs-pdf/p1212.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) and splitting them into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 8192 tokens, which roughly translates to ~32,000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "#from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 81 documents loaded is 5889 characters.\n",
      "After the split we have 560 documents more than the original 81.\n",
      "Average length among 560 documents (after split) is 912 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had 3 PDF documents which have been split into smaller ~500 chunks.\n",
    "\n",
    "Now we can see how a sample embedding would look like for one of those chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference endpoint: Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\bedrock.py\u001b[0m in \u001b[0;36m_embedding_func\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;31m# invoke bedrock API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             response = self.client.invoke_model(\n\u001b[0m\u001b[0;32m    140\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1004\u001b[0m             \u001b[0mapply_request_checksum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m             http, parsed_response = self._make_request(\n\u001b[0m\u001b[0;32m   1006\u001b[0m                 \u001b[0moperation_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[1;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[0;32m    118\u001b[0m         )\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_retries_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattempts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         success_response, exception = self._get_response(\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\endpoint.py\u001b[0m in \u001b[0;36mcreate_request\u001b[1;34m(self, params, operation_model)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mevent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'request-created.{service_id}.{operation_model.name}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             self._event_emitter.emit(\n\u001b[0m\u001b[0;32m    133\u001b[0m                 \u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\hooks.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[0maliased_event_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_event_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_emitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maliased_event_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\hooks.py\u001b[0m in \u001b[0;36memit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \"\"\"\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_emit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\hooks.py\u001b[0m in \u001b[0;36m_emit\u001b[1;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Event %s: calling handler %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m             \u001b[0mresponses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\signers.py\u001b[0m in \u001b[0;36mhandler\u001b[1;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# Don't call this method directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\signers.py\u001b[0m in \u001b[0;36msign\u001b[1;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\botocore\\auth.py\u001b[0m in \u001b[0;36madd_auth\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNoCredentialsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[0mdatetime_now\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoCredentialsError\u001b[0m: Unable to locate credentials",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2995fb43defa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mStopExecution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-2995fb43defa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msample_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbedrock_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample embedding of a document chunk: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Size of the embedding: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\bedrock.py\u001b[0m in \u001b[0;36membed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mEmbeddings\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedding_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\agust\\anaconda3\\lib\\site-packages\\langchain_community\\embeddings\\bedrock.py\u001b[0m in \u001b[0;36m_embedding_func\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"embedding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error raised by inference endpoint: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_normalize_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by inference endpoint: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "    print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "\n",
    "except ValueError as error:\n",
    "    if \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the similar pattern embeddings could be generated for the entire corpus and stored in a vector store.\n",
    "\n",
    "This can be easily done using [FAISS](https://github.com/facebookresearch/faiss) implementation inside [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html) which takes  input the embeddings model and the documents to create the entire vector store. Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation) helps us with that.\n",
    "\n",
    "**⚠️⚠️⚠️ NOTE: it might take few minutes to run the following cell ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "Now that we have our vector store in place, we can start asking questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"Is it possible that I get sentenced to jail due to failure in filings?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step would be to create an embedding of the query such that it could be compared with the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embedding = vectorstore_faiss.embedding_function.embed_query(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this embedding of the query to then fetch relevant documents.\n",
    "Now our query is represented as embeddings we can do a similarity search of our query against our data store providing us with the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the relevant documents, it's time to use the LLM to generate an answer based on these documents. \n",
    "\n",
    "We will take our inital prompt, together with our relevant documents which were retreived based on the results of our similarity search. We then by combining these create a prompt that we feed back to the model to get our result. At this point our model should give us highly informed information on how we can change the tire of our specific car as it was outlined in our manual.\n",
    "\n",
    "LangChain provides an abstraction of how this can be done easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #1: Using LangChain with RetrievalQA\n",
    "You have the possibility to use the wrapper provided by LangChain which wraps around the Vector Store and takes input the LLM.\n",
    "This wrapper performs the following steps behind the scences:\n",
    "- Take the question as input\n",
    "- Create question embedding\n",
    "- Fetch relevant documents\n",
    "- Stuff the documents and the question into a prompt\n",
    "- Invoke the model with the prompt and generate the answer in a human readable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note: As an exercise. If you have time, update the cell above so that it uses the \"new/appropriate\" version/style of code, so that the deprication issue/warning is resolved.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That answer shows that full response, which includes a lot of noise. Zeroing in on primary aspect of the natural language message that we might return to the end user ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_ww' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d8796ff5e2a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_ww\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'print_ww' is not defined"
     ]
    }
   ],
   "source": [
    "print_ww(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a different question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_2 = \"What is the difference between market discount and qualified stated interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_2 = qa({\"query\": query_2})\n",
    "# show the full response\n",
    "print_ww(answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That answer shows that full response, which includes a lot of noise. Zeroing in on primary aspect of the natural language message that we might return to the end user ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ww(answer_2['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example # 2\n",
    "Now let's have another look at using [RetrievalQA](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) where you can customize how the documents fetched should be added to prompt using `chain_type` parameter. Also, if you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using `return_source_documents` which returns the documents that are added to the context of the LLM prompt. `RetrievalQA` also allows you to provide a custom [prompt template](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html) which can be specific to the model.\n",
    "\n",
    "In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "query = \"Is it possible that I get sentenced to jail due to failure in filings?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Assignment Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't panic! This is not as difficult as it might first seem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the notebook above as a guide and/or starting point, consider and explore the following questions and tasks. As with most things in life, you'll get the most out this exercise by putting in a reasonable amount of effort. That effort may in in research and/or in writing code.\n",
    "\n",
    "Put all your answers into the notebook that you are going to submit as your completed assignment.\n",
    "For each Task, there is an ask to consider the results and note your findings, please these notes in to the notebook.\n",
    "\n",
    "This is a great opportunity to use or develop your markdown skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 1** (for everyone) \n",
    "\n",
    "Change the base content used here (the PDFs in the data folder) to be content that is meaningful to you in some way. It might be that it relates to the business domain that you are interested in, or on a topic that interests you. \n",
    "\n",
    "For this task, your content should be approximately 15 pages of text. That could be in a single document or in multiple documents. Related to that content create 10 questions or so. These questions will be your initial test set that you will use to determine the quality of your RAG solution.\n",
    "\n",
    "Upload your content and update the notebook to create a FAISS index of your documents, and update the question examples to use a subset of your questions.\n",
    "\n",
    "Take a look at the outputs and consider the solutions accuracy. The notebook at this point is your baseline. You might want to make a snapshot of it before you progress further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 answer**\n",
    "Two documents were uploaded to the folder \"./newdata\". The documents:  _'WiredScore-Factsheet-NorthRowTowers.pdf'_ and _'SmartScore-Factsheet-NorthRowTowers.pdf'_ contain the summary of the result achived for the certifications WiredScore and SmartScore.\n",
    "\n",
    "The questions are:\n",
    "1. What WiredScore level did North Row Towers buidlign achieved?\n",
    "2. What is the overview Set-up experience in North ow Towers?\n",
    "3. What is the overview Future-ready experience in North ow Towers?\n",
    "4. What are the connected building highlights at North Row towers about Mobile and Internet?\n",
    "5. What are the connected building highlights at North Row towers about Resiliency?\n",
    "6. What is WiredScore?\n",
    "7. What SmartScore level did North Row Towers buidlign achieved?\n",
    "8. What is the overview Sustainabilty experience in North ow Towers?\n",
    "9. What is the overview Access and Navigation experience in North ow Towers?\n",
    "10. What are the smart building highlights at North Row towers about Health and Wellbeing?\n",
    "11. What are the smart building highlights at North Row towers about Saftey and Security?\n",
    "12. What is SmartScore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 1** Baseline Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** Baseline Evaluation \n",
    "\n",
    "We want to improve the quality of the output of the solution. We know that we have not done any tuning for the content so far, so our intuition is that we can do better.  Before we start experimenting to improve the quality, we really need to objectively measure the quality that we have.\n",
    "\n",
    "You have a set of test questions (from task 1) to help drive your evaluation.\n",
    "\n",
    "You decide that you will first test that retrieval aspect of the solution. You want a metric for if the right chunks of content being returned for your test queries.\n",
    "\n",
    "A. Describe the ground-truth data would your create so that you can measure this? (hint: test query, document chunk)\n",
    "\n",
    "B. Describe how you might determine success or failure of retrieval for a test query\n",
    "\n",
    "C. Is your determination of success/failure binary (True/False) or graded 0.0 - 1.0 or both? Describe the reasoning for your choice.\n",
    "\n",
    "D. Describe your approach for calculating an aggregate metric (or metrics) for measuring performance of your test set with this base configuration. And, outline the reasoning for your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 answer**\n",
    "\n",
    "**A. Measuring ground-truth**.\n",
    "The ground truth would be the chuck section that would answer the questions. For instance for the questions _\"What is the overview Future-ready experience in North tow Towers?\"_ we will expect that the answer consider the chunk: \t_\"To meet your needs far into the future, the building is equipped with spare capacity in the risers, point of entry and telecommunications room. This ensures the building can accommodate new and emerging technologies, providing you with the flexibility and adaptability you require.\"_\n",
    "\n",
    "**B. Determining success or failure for a test query**.\n",
    "DeepEval is a popular open source framework that to evalaute LLM perfomance. For RAG they propose [the RAGAS metric](https://docs.confident-ai.com/docs/metrics-ragas). RAGAS formed as the average of four differnt metrics, which are:\n",
    "\n",
    "- **Answer relevancy**: \"If someone asks you who the first man to walk on the moon was, and you answer Columbus was the first to find America, your answer is entirely irrelevant\". The metric run an LLM and look for the percentage (%) of retrieved chucks that are relevant to the query. [see more here](https://docs.confident-ai.com/docs/metrics-answer-relevancy)\n",
    "- **Contextual precession**: \"Suppose you ask someone about who the president of the US was during the Apollo 11 mission. In that case, that person says, “Obama was president when Bin Laden was eliminated, and John F Kennedy was president when Amstrong stepped on the Moon.” The person has the answer but puts forward an irrelevant one first\". This metric check whether your retrieval system ranks the relevant documents higher than the others. [see more here](https://docs.confident-ai.com/docs/metrics-contextual-precision)\n",
    "- **Contextual recall**: Contextual recall measures whether the retrieval context is sufficient to answer the problem independent of the rank. [see more here](https://docs.confident-ai.com/docs/metrics-contextual-recall)\n",
    "- **Faithfulness**:This metric purely evaluates the ability of the final LLM to produce the output with the context provided. A high score is achieved if there is not any contradiction between the output and the retrieved context. [see more here](https://docs.confident-ai.com/docs/metrics-faithfulness)\n",
    "\n",
    "**C. Metric form**.\n",
    "All these metrics can be ealisy converted to fail/passed if they is above a given treshold and if using deepdeval library the the LLM's reason can be retrived too. Nevertheless, given that we do not know for this case what treshold is acceptable we will use numbers 0.0 - 1.0 and develop our intuition of what is a good treshold.\n",
    "\n",
    "**D. Aggregated Metric**.\n",
    "The RAGAS framework and uses the average of the four previous metrics. \n",
    "Nevertheless each of these metrics measure different parts of the RAG solution:\n",
    "- **Answer relevancy**: Relevant data retrived\n",
    "- **Contextual precession**: Correct rank of the data retrieved\n",
    "- **Contextual recall**: Completness of the data retrieved\n",
    "- **Faithfulness**: overall test of the solution including Retrival and the final LLM model\n",
    "\n",
    "Given that in this part we will focus on tunnig the retrival part of the solution, **we will only use the metrics _Answer relevancy_,  _Contextual precession_ and aggregate them as an average as our final metric**. By doing so we tackle the main goal \"retival\" while developing faster and test cheap as each metric needs to call an LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2** (strongly encouraged, but optional) \n",
    "\n",
    "Implement, and perhaps refine, your evaluation methods and capture your baseline metrics, by running your set of test cases.\n",
    "\n",
    "Briefly summarize your results (the baseline metrics) and any intuitions that you have about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 2** Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** Chunking\n",
    "\n",
    "You know from your research that a critical factor in RAG solutions is how the document corpus is chunked (prior to embedding being created for the chunk, etc). Consider your content and the chunking options that are commonly used in RAG solutions. \n",
    "\n",
    "Choose 2 or 3 chunking options/variants that you believe might be better than baseline option that is configured in this notebook, and which are therefore worth experimenting with.\n",
    "\n",
    "For each option that you choose, briefly outline why you think that might provide better results for your solution and particular document set.\n",
    "\n",
    "For consideration: \n",
    "\n",
    "https://www.pinecone.io/learn/chunking-strategies/\n",
    "\n",
    "https://python.langchain.com/docs/how_to/semantic-chunker/\n",
    "\n",
    "https://blog.langchain.dev/a-chunk-by-any-other-name/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 answer**\n",
    "\n",
    "Playing with the original data we have identified that the main difficulty for the retrival is that PDFs are summaries of the certification and to make it easier for readers it uses tables and contaires to show the information. So, in the current form the fixed leght chuck loses the semnatics contained in the structure. Therefore the chucks can use this structure to be enriched with elements such as: _Titles, Column Headers, Row Headers_. \n",
    "\n",
    "Some interesting chunking options for that can be:\n",
    "1. Document Specific Splitting.\n",
    "    - [Unstructured](https://app.unstructured.io/) has a model for tables that we can try\n",
    "    - Use Amazon's Textract to identify tables and other semantic in the text. There is a tutoria created by [AWS](https://github.com/aws-samples/layout-aware-document-processing-and-retrieval-augmented-generation)\n",
    "2. Sructured Chunking. The idea is to use the strucure of the document to split the document.The pdf we are using can also be retrieved in HTML format directly from the web. Then we can use the HTML to retrive the titles header etc.\n",
    "3. Contextual chunking. [Saad-Falcon et all. (2023)](https://arxiv.org/pdf/2309.08872v1) proposed to convert pdf into HTML to identify the structure and then add that stucure information to the chuck metadata. LangChain have an example of this in [this code](https://github.com/rajib76/langchain_examples/blob/main/examples/how_to_parse_pdf_with_complex_tables.py).   \n",
    "3. Semantic Chunking. The idea is to use an embedding that checks the embeddin differences of senteces to decide if the sentences should be in the same chunk or should be in different chunks. LangChain has a [tutorial](https://python.langchain.com/docs/how_to/semantic-chunker/) that can be used as reference.\n",
    "\n",
    "Aditionally the data should be pre-processed. For instance '\\n' (next line) is used more than normal as the text was forces to fit an space, therefore this can be simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3** (strongly encouraged, but optional) \n",
    "\n",
    "Update the implementation to use one or more of your chuncking options.\n",
    "\n",
    "For the evaluation process, you may need to create an new ground-truth dataset of each chunking option. The good news is that we're only using 10 test cases or so.\n",
    "\n",
    "Run evaluation methods and capture the revised metrics. \n",
    "\n",
    "Briefly summarize your results (the new metrics) and any intuitions that you have about the results.\n",
    "\n",
    "For consideration: \n",
    "\n",
    "https://github.com/aws-samples/amazon-bedrock-claude-2-and-3-with-langchain-popular-use-cases/blob/main/Amazon%20Bedrock%20%26%20Langchain%20Sample%20Solutions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** Embeddings\n",
    "\n",
    "You know that one of the major factors that will impact the accuracy of the retrieval solution is the embeddings model that is used to encode the document corpus and the questions that get asked. \n",
    "\n",
    "Your customer has asked you to select two alternative models to the one in the baseline solution, from the set of embeddings models available to you in Amazon Bedrock and from Voyagai. Your goal is suggest two that will provide better retrieval performance that the baseline.\n",
    "\n",
    "Choose two models and describe your reasoning on why each of those models might provide better results. \n",
    "\n",
    "As with much of generative AI, the best option will need testing with the content and likely cannot be fully determined from research/experience alone, but you goal is to provide a brief rationale for your recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 Answer**\n",
    "One great way to benchmark the different retrieval models is using the [Huggigface leaderboard](https://huggingface.co/spaces/mteb/leaderboard) where it can be found the best embedding model for certain task. In the \"Retrieval\" tab we can see these models, the most similar task to our is _QuoraRetrival_. \n",
    "Nevertheless not all of them are available in bedrock (eu-west-2). So from the Cohere models and vogageai, we suggest to try the models: **embed-multilingual-v3.0** with 88.92 in the _QuoraRetrival_ task and given that the use case can be in different languages, this model can be used for that too. From Vogageai we sugest **voyage-multilingual-2** due to the multilingual capabilites.\n",
    "\n",
    "It worth mentioning that aditionnally to Embeddings there are another type of methods to transform the data to be called later, these are the _keyword-frequency based methods_ like TF-IDF and BM-25. These methods shine when we lookfor chunks that match particular words. For our use case we will prefer embeddings model as we will not need matching keywords apart from ensuring that the chunks come from the particlar building report, which we can handle by using _metadata filtering_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4** (strongly encouraged, but optional)\n",
    "\n",
    "Using your best (perhaps only) chunking strategy, experiment with applying the embeddings models that you recommended.\n",
    "\n",
    "Run evaluation methods and capture the revised metrics. \n",
    "\n",
    "Briefly summarize your results (the new metrics) and any intuitions that you have about the results.\n",
    "\n",
    "For consideration:\n",
    "\n",
    "https://python.langchain.com/docs/integrations/text_embedding/voyageai/\n",
    "\n",
    "https://docs.voyageai.com/docs/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 4** K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4** K\n",
    "\n",
    "How many matching results, K, fed into your LLM (in the augmented prompt) is going to have a impact on the cost, latency and the quality of output generated by the solution. In an ideal world, there would only be one chunk, K = 1, and that chunk would have all the information that is needed for the users question, and would be right-sized, with little data/text that is not relevant to the question.\n",
    "\n",
    "A. K is set to 3 for the baseline solution. If you set this to 1, 2, or 4, or 5 how will this change your evaluation metrics?\n",
    "\n",
    "B. If your retrieval metrics are well-designed, there is likely very little change if K is set to 3, 4 or 5. Why is this the case?\n",
    "\n",
    "C. It was noted (above) that we do not want K to be large, how might we test what is the best size for K? Hint: it goes beyond just testing retrieval metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 Answer**\n",
    "\n",
    "A. In the original case the result doesnt change the Faithfulness metric as the model answer the same independent of K\n",
    "\n",
    "B. the faithfulness metric changed little because the model gives prioriry to the first retrieved chunks. That is why _Contextual Presision_ is such an impotant metric as it shows this issue\n",
    "\n",
    "C. Setting K works as tradeoff between giving more context and providing irrelevant chunks to the LLM. Then, _Faithfulness_ is the most important metric to find the K value, as we need to test if the final response is correct or not independent of the value of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5** (strongly encouraged, but optional)\n",
    "\n",
    "Experiment with different values of K (1, 2, 3, 4). \n",
    "\n",
    "Evaluate and capture the retrieval metrics for each value of K.\n",
    "\n",
    "Briefly summarize your results (the new metrics) and any intuitions that you have about the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 5** Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5** Re-ranking\n",
    "\n",
    "Adding a re-ranking model to our RAG pipeline helps us 1/ provide a better set of document chunks for RAG, and 2/ may allow us to reduce the number of chunks that are used for augmenting the prompt.\n",
    "\n",
    "Breifly explain how re-ranking helps with with points 1 and 2 above.\n",
    "\n",
    "Given the characteristics of your document content and RAG pipeline we have here, can you recommend one or two re-reranking models to experiment with? Brielfy outline the rationale for your recommendation.\n",
    "\n",
    "For consideration:\n",
    "\n",
    "https://blog.voyageai.com/2024/03/15/boosting-your-search-and-rag-with-voyages-rerankers/\n",
    "\n",
    "https://developer.nvidia.com/blog/enhancing-rag-pipelines-with-re-ranking/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 answer**\n",
    "\n",
    "Rerankers work by processing the raw retrieved text in response to the actual query, using transformer models to analyze the document in the context of the specific query. This method minimizes information loss and enhances relevance, as the evaluation is tailored to the individual query rather than a precomputed, generalized interpretation.\n",
    "\n",
    "Rerankers are precises but it comes at the cost of speed. Unlike embeddings where the computational heavy lifting is done ahead of time, rerankers require running a full transformer model computation for each query-document pair during the query time. This results in slower response times but significantly higher accuracy and relevance.\n",
    "\n",
    "[Wang et al. (2024)](https://arxiv.org/pdf/2407.01219) tested different parameters for the RAG solution. For rerankig activiteis they found that \n",
    "_\"monoT5 as a comprehensive method balancing performance and efficiency. RankLLaMA is suitable for achieving the best performance, while TILDEv2 is ideal for the quickest experience on a fixed collection\"_. \n",
    "Given our case I would suggest using monoT5 or RankLLaMA as lattency is not a big problem for our client. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6** (encouraged, but optional)\n",
    "\n",
    "Add re-ranking to the RAG solution. Using the Voyagai models will likely be easiest. Experiment with a couple re-ranking models.\n",
    "\n",
    "After adding the re-ranker you will likely find that you get best evaluation results by having a value of 3 or 4 K for retreival from the FAISS vector database, and then taking 2 or 3 of the re-ranked chunks.  \n",
    "\n",
    "Briefly summarize your results (the new metrics) and any intuitions that you have about the results.\n",
    "\n",
    "For consideration:\n",
    "\n",
    "https://python.langchain.com/docs/integrations/document_transformers/voyageai-reranker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7** (encouraged, but optional)\n",
    "\n",
    "Likely the evaluation metrics that you are using thus far is not sensitive to which rank in the set of documents the positive chunk hit/hits are. Order of the chunk, aka rank, is important as 1/ it will likely help the LLM produce a better output, 2/ if the retrieval system consistently returns chunk results in an optimal order, it allows us to more aggressively prune the retrieval results before giving the chunks to the LLM.\n",
    "\n",
    "Add to the set evaluation metric(s) to have a metrics that has values correct order. The new metric will produce the highest value for correct answers, chunk(s), in the top rank(s), lower values for chunks in none top ranks. This metric better reflects our retrieval system objectives.\n",
    "\n",
    "Once you have this, redo task 6, and see what the optimal configuration is for K and the re-ranking configuration.\n",
    "\n",
    "Briefly summarize your results (the new metrics) and any intuitions that you have about the results.\n",
    "\n",
    "For consideration:\n",
    "\n",
    "https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems\n",
    "\n",
    "https://towardsdatascience.com/ranking-evaluation-metrics-for-recommender-systems-263d0a66ef54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 6** Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6** Distance\n",
    "\n",
    "An aspect of RAG tuning that easy to experiment with, but is often overlooked it the distance measure that is used to compare the embedding of the query, with the embedding of the documents.\n",
    "\n",
    "What is the distance measure/method that is used in the baseline implementation?\n",
    "\n",
    "Your customer wants to experiment with one or two other distance measure for this pipeline. Which measures are you going to recommend? Provide a brief rationale of your recommendation.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html\n",
    "\n",
    "https://python.langchain.com/docs/integrations/vectorstores/faiss/\n",
    "\n",
    "https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances\n",
    "https://www.pinecone.io/learn/series/faiss/faiss-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 Answer**\n",
    "Distance metrics are essential for measuring the proximity between data points, enabling the effective identification of similar objects during the retrieval stage. By employing advanced distance metrics, we can enhance the precision of our search results, thereby improving the overall accuracy and reliability of the RAG system.\n",
    "\n",
    "The default distance metric used by the FAISS retriever is **Cosine similarity**, which can be modified at the initialization stage by adjusting the **_distance_strategy_** parameter.\n",
    "\n",
    "The following are the [possible values for FAISS retrieval](https://api.python.langchain.com/en/latest/utilities/langchain_aws.utilities.utils.DistanceStrategy.html#langchain-aws-utilities-utils-distancestrategy), along with their meanings (as referenced by [Saika (2024)](https://medium.com/@parikshitsaikia1619/unlock-rags-potential-with-distance-metrics-and-rerankers-42df4f171f5a)):\n",
    "\n",
    "1. **COSINE**: Cosine similarity evaluates the cosine of the angle between two vectors originating from the same point. It ranges from -1 (opposite directions) to 1 (same direction), with 0 indicating orthogonality. This metric is particularly beneficial for measuring the direction of vectors rather than their magnitude, making it ideal for comparing texts or documents where content relevance is more significant than length.\n",
    "\n",
    "2. **EUCLIDEAN_DISTANCE**: This measurement calculates the straight-line distance between two points (vectors) in space, determining the square root of the sum of squared differences across corresponding vector elements. It is intuitive and particularly effective for scenarios where the physical distance matters, such as retrieving visually similar images. For example, queries like, _“Give me 10 locations for holidays with activities similar to those in Marveya, Spain,”_ benefit from this metric.\n",
    "\n",
    "3. **DOT_PRODUCT**: The dot product assesses how much one vector aligns with another, considering both their magnitude and direction. A higher dot product indicates a stronger similarity in both features and alignment. This metric is especially useful for retrieving specific information, such as drug compositions or barcodes. It suits queries like, _“Give me 10 locations for holidays with temperatures between 25 and 30 degrees Celsius in summer.”_\n",
    "\n",
    "4. **MAX_INNER_PRODUCT**: This metric retrieves the vector with the maximum inner product, highlighting the most similar vectors in terms of both content and context. This method is useful in contexts where we want to retrieve items that are not only similar but also relevant in a more nuanced way, representing a stronger association between the query and the results.\n",
    "\n",
    "5. **JACCARD**: Jaccard Similarity measures the size of the intersection divided by the size of the union of two sets. It assesses the similarity between two sets (or documents) based solely on the overlap of their elements—typically words or terms. It doesn't consider the context or meaning of the terms, making it best suited for keyword-based searches, where the presence or absence of specific words is more critical than their overall semantic content.\n",
    "\n",
    "**Recommendation**: For our use case, where we aim to retrieve descriptions of buildings—such as, _“How is the setup in Two Towers?”_—I recommend utilizing **Cosine similarity** combined with **Dot Product**. This approach will help us retrieve relevant information specific to the _Two Towers_, while we can further refine our results using metadata filters to exclude unrelated content, such as _Black Tower_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8** (encouraged, but optional)\n",
    "\n",
    "Experiment with the setting the distance method for the vector database comparsion to the alternatives that you suggested above.\n",
    "\n",
    "Briefly summarize your results (the new metrics) and any intuitions that you have about the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 7** - Wrapping up the Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7** - Wrapping up the Retrieval System\n",
    "\n",
    "At this point we have completed a full iteration of tuning of the data retrieval aspect of our RAG solution. \n",
    "\n",
    "A. Briefly outline two or three insights from this exercise\n",
    "\n",
    "B. Do you have further suggestions for how the accuracy of the retrieval system could be further tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 answer**\n",
    "\n",
    "Throughout this exercise, we have recognized the critical interplay between various components of our RAG solution. Each aspect, from the initial data collection to the retrieval methods, affects the overall effectiveness of the system. Understanding how changes in one area can ripple through the system has been illuminating.\n",
    "\n",
    "One of the most significant insights has been the impact of the chunking strategy on our retrieval results. The choice between larger, context-preserving chunks and smaller, precise chunks fundamentally alters the nature of the data being processed. For example, while larger chunks maintain contextual integrity, enabling the model to capture broader meanings, smaller chunks offer granularity which may improve specific queries. This has emphasized the need to carefully consider how we segment our data before presenting it to the foundational model.\n",
    "\n",
    "\n",
    "About suggestions ways for Further Tuning the Accuracy of the Retrieval System I can see:\n",
    "\n",
    "1. **Implementing Filters**: As previously mentioned, incorporating filters into the retrieval process is vital. This will ensure that we do not retrieve or display data from other clients or irrelevant buildings. By configuring these filters effectively, we can refine our results to show only the most pertinent information, thereby enhancing user experience and data security.\n",
    "\n",
    "2. **Source Attribution**: Including the source of the data in the retrieval results would also be beneficial. By providing users with the origin of the information, they can access the original deliverables if they wish to delve deeper into the context. This not only enhances transparency but also builds trust in our solution, as users can verify data and its relevance to their specific needs.\n",
    "\n",
    "3. **User Feedback Loop**: Establishing a user feedback mechanism can provide ongoing insights into the effectiveness of the retrieval system. By allowing users to rate the relevance and accuracy of retrieved results, we can continuously fine-tune our algorithms and strategies based on real-world interactions. This iterative process can lead to significant long-term improvements in the accuracy and relevance of our searches.\n",
    "\n",
    "By implementing these suggestions, we can further enhance the accuracy and reliability of our RAG solution, ensuring that it meets user expectations and provides valuable insights efficiently."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
